{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b490a809",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "Goal: create clean train/test splits with proper scaling while preventing data leakage. No resampling here; imbalance is handled later.\n",
    "\n",
    "Steps:\n",
    "- Load raw data\n",
    "- Stratified train-test split (`random_state=42`)\n",
    "- Scale features using `StandardScaler` (fit on train only)\n",
    "- Save processed splits to `data/processed`\n",
    "\n",
    "Outputs:\n",
    "- X_train_scaled.csv, X_test_scaled.csv\n",
    "- y_train.csv, y_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bec412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature names (first 5): ['num__TransactionID' 'num__Amount' 'num__MerchantID'\n",
      " 'cat__TransactionDate_00:35.5' 'cat__TransactionDate_00:35.6']\n",
      "Cleaned feature names (first 5): ['num__TransactionID', 'num__Amount', 'num__MerchantID', 'cat__TransactionDate_00_35_5', 'cat__TransactionDate_00_35_6']\n",
      "Total features: 375\n",
      "\n",
      "Saved processed files to: ..\\data\\processed\n",
      "X_train shape: (80000, 6)\n",
      "X_test shape: (20000, 6)\n",
      "Saved preprocessing metadata to: ..\\results\\metrics\\02_preprocessing_metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((80000, 6), (20000, 6))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Use path relative to this notebook (../data/...) so it works even if run from notebooks folder\n",
    "RAW_PATH = Path('../data/raw/credit_card_fraud_dataset.csv')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup results directories\n",
    "RESULTS_DIR = Path('../results')\n",
    "METRICS_DIR = RESULTS_DIR / 'metrics'\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "# Target column in this dataset is 'IsFraud'\n",
    "data = pd.read_csv(RAW_PATH)\n",
    "X = data.drop(columns=['IsFraud'])\n",
    "y = data['IsFraud']\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Preprocessor: scale numeric, one-hot encode categorical (fit on train only)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names and clean them for ML library compatibility (especially LightGBM)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Clean column names: replace special characters with underscores\n",
    "# This fixes issues with LightGBM not accepting characters like ':', '.', etc.\n",
    "def clean_column_names(columns):\n",
    "    \"\"\"Replace special characters with underscores for ML library compatibility\"\"\"\n",
    "    return [re.sub(r'[^\\w]', '_', col) for col in columns]\n",
    "\n",
    "feature_names_clean = clean_column_names(feature_names)\n",
    "\n",
    "print(f\"Original feature names (first 5): {feature_names[:5]}\")\n",
    "print(f\"Cleaned feature names (first 5): {feature_names_clean[:5]}\")\n",
    "print(f\"Total features: {len(feature_names_clean)}\")\n",
    "\n",
    "# Persist processed data and preprocessor\n",
    "pd.DataFrame(X_train_processed.toarray() if hasattr(X_train_processed, \"toarray\") else X_train_processed,\n",
    "             columns=feature_names_clean).to_csv(PROCESSED_DIR / \"X_train_scaled.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(X_test_processed.toarray() if hasattr(X_test_processed, \"toarray\") else X_test_processed,\n",
    "             columns=feature_names_clean).to_csv(PROCESSED_DIR / \"X_test_scaled.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(PROCESSED_DIR / \"y_train.csv\", index=False)\n",
    "y_test.to_csv(PROCESSED_DIR / \"y_test.csv\", index=False)\n",
    "joblib.dump(preprocessor, PROCESSED_DIR / \"preprocessor.joblib\")\n",
    "\n",
    "print(f\"\\nSaved processed files to: {PROCESSED_DIR}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Save preprocessing metadata\n",
    "preprocessing_metadata = {\n",
    "    'original_shape': {'train': list(X_train.shape), 'test': list(X_test.shape)},\n",
    "    'processed_shape': {'train': [X_train_processed.shape[0], len(feature_names_clean)], \n",
    "                       'test': [X_test_processed.shape[0], len(feature_names_clean)]},\n",
    "    'numeric_features': list(numeric_features),\n",
    "    'categorical_features': list(categorical_features),\n",
    "    'total_features_after_encoding': len(feature_names_clean),\n",
    "    'test_split_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'class_distribution': {\n",
    "        'train': {'non_fraud': int((y_train == 0).sum()), 'fraud': int((y_train == 1).sum())},\n",
    "        'test': {'non_fraud': int((y_test == 0).sum()), 'fraud': int((y_test == 1).sum())}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(METRICS_DIR / '02_preprocessing_metadata.json', 'w') as f:\n",
    "    json.dump(preprocessing_metadata, f, indent=2)\n",
    "print(f\"Saved preprocessing metadata to: {METRICS_DIR / '02_preprocessing_metadata.json'}\")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8858b08",
   "metadata": {},
   "source": [
    "Note: Resampling to address imbalance is deferred to `03_imbalance_handling.ipynb` to keep data leakage risks low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
